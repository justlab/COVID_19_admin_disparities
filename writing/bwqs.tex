\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Negative Binomial }
\author{Elena Colicino\\ Nicolo Foppa Pedretti}
\date{October 2020}

\begin{document}

\maketitle

\section{Bayesian Weighted Quantile Sum Regression with outcomes following a Negative Binomial density distribution}

Here we provide a description of Bayesian Weighted Quantile Sum (BWQS) regression when the outcome variable $Y$ has a negative binomial density distribution.\\
The classical formulation of the  negative binomial density distribution is:

$$\mathcal{NB}(y | \mu, \phi)  = \frac{\Gamma{(y + \phi)}} {\Gamma{(\phi)}\Gamma{(y+1)}}
\left( \frac{\mu}{\mu+\phi} \right)^{y} \left(
\frac{\phi}{\mu+\phi} \right)^{\phi}$$

where $\mu \in \mathbb{R}^+$, $\phi \in \mathbb{R}^+$ and $y \in \mathbb{N}$. \\
The mean and variance of a random variable $Y \sim \mathcal{NB}(y | \mu, \phi)$ are:
$$ \mathbb{E}[Y] = \mu \ \ \
\text{ and } \ \ \ \text{Var}[Y] = \mu + \frac{\mu^2}{\phi}$$

We included the negative binomial distribution with $\eta = \log(\mu)$ where $\eta \in \mathbb{R}$ in the BWQS regression framework, so that the BWQS regression model has the following form: $$\mathcal\eta = \beta_0 + \beta_1*\text{BWQS} + \delta^{T}\textbf{X}$$

where $\beta_0$ is the intercept; $\beta_1$ is the coefficient mapped to the BWQS index of $N_C$ mixture components previously ranked in quantiles, $q$; $\text{BWQS}$ index is $\sum_{j = 1}^{N_C}w_{j}q_{ij}$ with weight $w_j$ for the $j$-th mixture component; and $\underline{\delta}$ is a vector of coefficients mapped to the $N_k$ covariates $\textbf{X}$.\\

The choice of the prior of the model is based on prior literature, prior definition of the BWQS regression and their properties of being conjugate:
\begin{center}
    $\beta_0,\beta_1  \sim \mathcal{N}(0, 100)$, \\
    $\delta \sim \mathcal{N}_{N_k}(0, 100*\mathbb{I}_{N_k})$, \\
    $\phi \sim inv\mathcal{\Gamma}(0.01, 0.01)$, \\
    $w \sim Dirichlet(1_{N_c})$ \\
\end{center}


\section{Dirichlet distribution}
The Dirichlet distribution $Dir(\textbf{$\alpha$})$ is a multivariate generalization of the Beta density distribution and it belongs to a family of continuous multivariate probability distributions parameterized by a vector \textbf{$\alpha$}. \\
The \textbf{$\alpha$} vector has the characteristics of the multinomial parameter, i.e. the components of the $\alpha$ vector ($\alpha_i$ for all $i$-th component) are positive reals and the sum of all components is equal to 1 ($\sum_{i = 1}^{I}\alpha_{i} $=1). This second characteristic implies that the estimates of all parameters are not independent, similarly with what we have with the multinomial distribution. For these characteristics, the Dirichlet distribution is commonly used as the prior for the multinomial distribution. \\
The Dirichlet distribution is also widely used as prior distribution because of its property of being conjugate, which means that the posterior distribution will be a Dirichlet with parameters α different from those of the priors. For this reason, this distribution has an easy computation and facilitates quantification of how much the prior beliefs have changed after including data in the model. 
In our case the Dirichlet is parametrized by a parameter vector $\alpha = (1,1,…,1)$, which assumes a uniform density distribution across the domain, implying a non-informative prior for all weights. Changes in the $\alpha$ parameter vector suggest stronger assumptions about the importance of each variable, which we don’t have a priori. In other words, the $\alpha$ parameter vector rules the shapes of the distribution; $\alpha_{i}=1$ assumes uniform distribution across the domain of the $i$-th mixture component, implying no prior information about its contribution to the overall mixture.

\phantom{To omit: Link from NB distribution and the regression:
distribution density can be expressed as:}\\
\fbox{\phantom{$$\mathcal{NB}_{log}(y | \eta, \phi)  = \mathcal{NB}(y | \exp(\eta), \phi)$$
Therefore, the}}\\


\end{document}
